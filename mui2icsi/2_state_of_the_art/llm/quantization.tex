Quantization refers to the process of representing a deep learning model's parameters (and sometimes activations) with reduced numerical precision. Instead of storing weights in high-precision floating-point formats such as \texttt{FP32} (32-bit), they are mapped to lower-precision formats like \texttt{FP16}, \texttt{INT8}, or even \texttt{INT4}. The main motivation is to reduce the model's size, memory footprint and computational cost, thereby making inference faster and enabling deployment on resource-limited devices. However, because quantization introduces rounding errors, careful methods are needed to mitigate accuracy loss.

There are several well-established approaches to quantization \cite{QUANTIZATION}:
\begin{compactitem}[\textbullet]
    \item In \textbf{Post-Training Quantization (PTQ)}, the model is trained in full precision and converted to a lower-precision format afterwards, making it quick to apply but potentially less accurate for sensitive models.
    \item \textbf{Quantization-Aware Training (QAT)} addresses this loss of accuracy by simulating low-precision effects during training so that the model learns to be robust to them, though this requires additional training time.
    \item \textbf{Dynamic Quantization} applies quantization to weights ahead of inference but computes activation ranges on the fly, offering a flexible solution without calibration data.
    \item Advanced strategies use \textit{mixed precision}, assigning higher precision to critical layers and lower precision elsewhere, or \textit{group-wise} (block-wise) quantization, where groups of weights share a scaling factor to improve performance at very low bit widths.
\end{compactitem}

Recently, \ac{llm}-specific methods have emerged. Algorithms such as \textit{Accurate Post-Training Quantization for \aclp{gpt} (GPTQ)} use second-order information to minimize quantization error \cite{GPTQ}, \textit{Activation-aware Weight Quantization for \ac{llm} Compression and Acceleration (AWQ)} targets weight distributions to preserve accuracy in low-bit settings \cite{AWQ}, and \textit{SmoothQuant} reduces activation outliers before quantization \cite{SMOOTHQUANT}. These methods are designed to balance compression and accuracy for large transformer-based architectures, making it possible to run models with billions of parameters on consumer-grade \acp{gpu} or even edge devices.
