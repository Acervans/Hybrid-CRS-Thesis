The enabling technology for modern conversational \ac{ai} is the \acl{llm}, a class of deep learning models based on the Transformer architecture \cite{SOTA-TRANSFORMERS}. These models have demonstrated a powerful ability to understand context, reason, and generate fluent natural language; with popular implementations such as ChatGPT \cite{CHATGPT}, Google Gemini \cite{GEMINI} or Claude \cite{CLAUDE}. However, when used in isolation, they suffer from several inherent limitations, including knowledge being confined to their static training data (a \textit{knowledge cut-off}) and a propensity to generate plausible but factually incorrect information (\textit{hallucinations}) \cite{SOTA-RAG-SURVEY}. To build robust and reliable applications, it is necessary to augment these models with external knowledge and capabilities. The following subsections detail the state-of-the-art frameworks and techniques used to achieve this.