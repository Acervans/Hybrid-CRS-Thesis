A diverse array of techniques has been developed for generating explanations in \aclp{rs}, each with different underlying principles and goals.

\paragraph{Feature and Review-Based Explanations}
One of the most direct methods for explainability involves leveraging the textual data associated with items. \textbf{Feature-level explanations} present key item attributes that align with a user's inferred preferences (e.g., ``...because you like the \textit{Action} genre''), and can be obtained through techniques such as SHAP (SHapley Additive exPlanations) or LIME (Local Interpretable Model-agnostic Explanations) \cite{SHAP, LIME}. \textbf{Review-level explanations} go a step further by extracting representative segments from user-generated reviews that highlight prominent aspects of an item. Early work in this area used topic modeling to identify latent topics in reviews that could be presented to users. More advanced models like the Neural Attentional Regression model with Review-level Explanations (NARRE) learn the ``usefulness'' of reviews to select the most informative ones as explanations \cite[Section 4.1]{CHAPTER:RS-HANDBOOK-NLP}.

\paragraph{Graph-Based Explanations}
With the rise of graph modelling in recommendation, a powerful paradigm has emerged that treats explainability as a graph reasoning problem. The core idea is that a compelling explanation can be represented as a path or subgraph that connects a user to a recommended item through a series of meaningful relationships \cite{SOTA-MODEL-AGNOSTIC-GRAPH-EXPLANATIONS}. For instance, a path like \texttt{User-[RATED]->Item1-[HAS\_ACTOR]->Actor- [ACTED\_IN]->Item2 (Recommended)} provides a transparent and interpretable reasoning trail (``You liked \texttt{Item1} which features \texttt{Actor}, who also appears in \texttt{Item2}''). This approach is highly effective because these paths represent concrete relationships in the data, enabling explainable recommendations while leveraging both user-item interactions and contextual knowledge from a domain-specific knowledge graph \cite{SOTA-CONTEXT-AWARE-EXP-KG}. This method is central to this thesis, relying on \texttt{FalkorDB}'s Cypher query capabilities to find collaborative and content-based reasoning paths that are then translated into natural language.

\paragraph{Counterfactual Explanations}
Inspired by causal inference, counterfactual reasoning explains a decision by identifying the minimal change to an input that would alter the outcome. The explanation answers the question, ``Why was item A recommended instead of item B?'' by identifying the critical feature that led to the decision \cite{SOTA-COUNTERFACTUAL-EXP-REC}. Frameworks like \texttt{CECR} (Counterfactual Explainable Conversational Recommender) integrate this technique directly into a \ac{crs}, using the generated counterfactuals not only as explanations but also as augmented training data to continuously improve the model's performance \cite{SOTA-CECR}.

\paragraph{LLM-Native Explanations}
The state-of-the-art in explainability for reommender systems spans towards \ac{llm}-generated explanations. \acp{llm} can act as surrogate models to interpret and mimic black-box recommenders, allowing for faithful natural language explanations \cite{SOTA-LLM-REC-EXPLAIN}, and incremental conversational frameworks integrate recommendation, explanation generation, and user feedback to enhance both accuracy and user-aligned explainability \cite{TOWARDS-EXP-CRS}.

This approach is often combined with other methods to ensure the explanations are grounded in factual evidence. The \textbf{Graph-RAG} paradigm is particularly relevant here; frameworks like \texttt{G-Refer} first retrieve explicit \acl{cf} paths from a graph and then use an \ac{llm} to translate this structured evidence into a coherent, natural language explanation \cite{G-REFER}. This ensures the explanation is not a post-hoc rationalization but is causally faithful to the underlying recommendation logic. This synergy between graph reasoning and \ac{llm} generation represents a noteworthy trend towards more trustworthy and effective explainable systems \cite{SOTA-RECSYS-EXPLAIN-KG, SOTA-CRS-EXPLAIN}.