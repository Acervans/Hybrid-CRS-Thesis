The core of the study involved participants completing three guided tasks, each designed to test a key functionality of the platform. After each task, participants were asked to rate its perceived difficulty on a 5-point Likert scale, where 1 was ``Very Easy'' and 5 was ``Very Difficult''. The tasks were as follows:

\begin{compactitem}[\textbullet]
    \item \textbf{Use Case 1: Agent Creation and Editing.} Participants were instructed to download a sample dataset, create a new recommender agent via the platform's multi-step creation wizard, and then edit the agent's metadata (e.g., name, description) to verify that the changes were reflected in the Agent Hub.
    \item \textbf{Use Case 2: First Conversation.} Participants were asked to initiate a conversation with the agent they had just created. The task involved engaging in a dialogue, providing preferences to the agent until recommendations were received, and then providing feedback on those recommendations.
    \item \textbf{Use Case 3: Retraining and Subsequent Conversation.} The final task required participants to use the ``Retrain'' function on their agent, which incorporates the feedback from the previous session into the expert model. After the agent finished retraining, they were asked to start a new conversation to see if the recommendations were updated.
\end{compactitem}