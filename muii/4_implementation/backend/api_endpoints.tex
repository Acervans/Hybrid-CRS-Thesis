The backend exposes a comprehensive RESTful \acs{api} using FastAPI. Request and response bodies are strictly typed and validated using Pydantic models defined in \texttt{schemas.py}, with model classes detailed in Appendix~\ref{AP:SCHEMAS}. This ensures data integrity and consistency between the frontend and backend. The main endpoints are grouped by functionality:

\begin{description}
    \item[Agent Management:] The \texttt{/create-agent} endpoint handles the complex, multi-part form data containing the agent configuration and uploaded dataset files. It triggers the asynchronous data processing and model training pipeline. The \texttt{/delete-agent} and \texttt{/retrain-agent} endpoints manage the lifecycle of an existing agent.

    \item[Conversational Workflow:] The \texttt{/start-workflow} endpoint initiates a new conversational session with an agent. It returns a \texttt{StreamingResponse} that allows the server to push events (such as \ac{llm} tokens or state changes) to the client in real-time. The \texttt{/send-user-response} endpoint allows the client to inject the user's reply back into the running workflow.

    \item[Chat History Management:] To manage chat histories, a set of endpoints were implemented. The \texttt{/create-chat-history}, \texttt{/get-chat-history}, \texttt{/append-chat-history}, and \\ \texttt{/delete-chat-history} endpoints handle the creation, retrieval, updating, and deletion of conversation logs, respectively.

    \item[\acs{llm} Proxy:] A general-purpose proxy endpoint, \texttt{/ollama/api/\{endpoint\}}, securely forwards requests to the Ollama server. This allows the frontend to interact with the \ac{llm} (e.g., to list available models or pull new ones) without exposing the Ollama service directly. It also intercepts chat requests to inject context from web search, by using the DDGS metasearch library \cite{DDGS-SEARCH}.

    \item[Data Utilities:] Helper endpoints like \texttt{/infer-column-roles} and \texttt{/pdf-to-text} provide utility functions to the frontend, offloading tasks like \ac{llm}-based inference or file parsing with PyMuPDF \cite{PYMUPDF} to the backend.
\end{description}
