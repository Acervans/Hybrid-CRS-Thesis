The automated data preprocessing pipeline is a critical component that enables the platform to ingest heterogeneous datasets and transform them into a standardized format. The core logic was designed as part of the Data Processing Module, is present in the majority of the data flow depicted in Figure~\ref{FIG:DATA_FLOWCHART}, and is one of the fundamental components in the creation of recommender agents. Therefore, the pipeline is designed to be robust and flexible, suitable for a wide variety of datasets while minimizing the need for manual intervention. This ensures that users can easily prepare their data for training recommender agents, regardless of its initial structure or quality.

The pipeline executes a series of steps upon receiving new dataset files from the user, starting from the frontend interface and ending with the processed data being saved to the backend:
\begin{compactenum}
    \item \textbf{Column Role Inference:} For user convenience, the system first attempts to infer the semantic roles of columns (e.g., user ID, item ID, rating). This is achieved through a specific \acs{api} endpoint using helper functions which send the column names to an \ac{llm} and ask it to return the roles in a structured JSON format.
    \item \textbf{Data Type Inference:} Similarly, a utility function infers the data type (e.g., \texttt{token}, \texttt{token\_seq}, \texttt{float} \& \texttt{float\_seq}) for each column by sending a sample of its values to the \ac{llm}. After this step, the user may modify column roles and types through the frontend interface, if necessary, before sending the dataset files and column configuration to the backend.
    \item \textbf{Normalization:} For the \texttt{ratings} column, a normalization function scales all values to a consistent range of [0, 5]. Furthermore, list-like columns are standardized into a space-delimited format (e.g., [``item1'', ``item2'', ``item3''] is converted to ``item1 item2 item3'').
    \item \textbf{Data Cleansing:} Data cleansing is performed by making use of the \textit{AutoClean} library \cite{AUTOCLEAN} to handle common data quality issues like duplicates and missing values.
    \item \textbf{Standardized Output:} Finally, the processed, cleaned, and standardized \texttt{DataFrames} are saved to a specific directory with a unique filename based on the dataset name and agent identifier, ready to be ingested by the recommendation and graph database modules.
\end{compactenum}

It is worth mentioning that, for the task of DataFrame processing (reading, filtering, manipulating...), \texttt{FireDucks} \cite{FIREDUCKS}---a library for efficient DataFrame operations by leveraging multithreading and compiler optimization---was used, in conjunction with \texttt{Pandas} \cite{PANDAS} for compatibility. \texttt{NumPy} \cite{NUMPY} was also utilized for optimized numerical operations and array manipulations.