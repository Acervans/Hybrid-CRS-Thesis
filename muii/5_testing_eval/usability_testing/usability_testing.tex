Usability testing of the HybridCRS platform demonstrated strong performance in user experience, usability, and recommendation quality. Eleven participants, primarily male and technologically experienced, completed three core tasks involving agent creation and conversation-based recommendations. Tasks were deemed relatively easy (1.00â€“1.55 out of 5, where 1 is very easy and 5 is very hard), and feedback highlighted the platform's intuitive design, relevance of recommendations, and stability.

The system achieved a high overall usability score, with a \ac{sus} score of 92 out of 100 and an average general rating of 9.36 out of 10. Users appreciated the combination of structured and conversational agents, though suggested improvements include better context interpretation, clearer labeling of recommendation sources, and onboarding guidance.

For full details of the usability tests, including methodology, constructive feedback and questionnaires, please refer to the complementary research thesis \cite{MUI2ICSI_THESIS}.