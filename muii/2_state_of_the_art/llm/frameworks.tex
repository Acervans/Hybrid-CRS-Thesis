Developing applications that effectively harness the power of \acp{llm} requires more than just access to a model. \Ac{llm} frameworks have emerged as essential tools that abstract away the complexity of building, training, and managing interactions with these models. These frameworks provide a structured approach to prompt engineering, state management, and the integration of various components like data sources and external \acp{api}.

Several frameworks have gained prominence in this area. LangChain \cite{LANGCHAIN} offers a general-purpose and comprehensive library for creating elaborate \ac{llm}-powered agents. LlamaIndex \cite{LLAMAINDEX}, conversely, adopts a more data-centric approach, providing robust tools specifically for connecting \acp{llm} to custom data sources, making it particularly well-suited for \ac{rag} pipelines and workflows. Other frameworks like Haystack \cite{HAYSTACK} also offer end-to-end solutions for building applications with \acp{llm}.

For the practical deployment and serving of local models, several tools are available. Platforms like Ollama \cite{OLLAMA}, based on llama.cpp \cite{LLAMA-CPP} simplify the process of running and managing open-source \acp{llm} on personal hardware. For performance-critical applications, inference engines such as vLLM \cite{VLLM} offer optimized memory management and throughput for serving \ac{llm} models efficiently.

In this project, LlamaIndex was chosen to manage the conversational workflow and data integration, for its lean but efficient feature set and overall modular pipeline. In addition, Ollama was used due to its simplified deployment of local models.