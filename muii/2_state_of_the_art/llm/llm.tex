The main component that enables the advanced conversational capabilities of the developed platform is the \acl{llm}. These models, built upon the transformative attention mechanism introduced by the Transformer architecture \cite{SOTA-TRANSFORMERS}, have demonstrated an unparalleled ability to comprehend, reason, and generate human-like text. This section provides a high-level overview of \acp{llm} and the key techniques used to augment their capabilities within our system.

To overcome the inherent limitation of their static training data, modern \ac{llm} applications employ advanced techniques to connect models with external, real-time information and tools. Two of the most prominent techniques are \ac{rag} and Function Calling. \ac{rag} allows an \ac{llm} to retrieve relevant information from an external knowledge base to ground its responses in factual, specific data \cite{SOTA-RAG-SURVEY}. Function Calling, on the other hand, enables the model to interact with external software and \acp{api}, allowing it to perform actions beyond simple text generation. A detailed exploration of these methods is conducted in the complementary thesis \cite{MUI2ICSI:SOTA}.

The field is populated by both proprietary models, such as those from OpenAI \cite{CHATGPT}, Google \cite{GEMINI}, and Anthropic \cite{CLAUDE}, and a rapidly growing ecosystem of open-source alternatives like the Qwen series of models \cite{QWEN}. This project leverages the flexibility of open-source models, which can be hosted locally for greater control and privacy.