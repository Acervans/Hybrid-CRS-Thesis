Raw data sourced from the real world is inherently noisy, inconsistent, and often incomplete. Before this data can be effectively used to train a recommender model or populate a knowledge base for an \ac{llm}, it must undergo a thorough preprocessing stage. This critical step, often referred to as data cleansing, involves a series of transformations to improve data quality and ensure its usefulness for downstream tasks.

Common preprocessing operations include handling missing or null values, identifying and removing duplicate entries, correcting structural errors, and standardizing data formats. For numerical data, normalization and outlier detection are often necessary to prevent skewed model behavior. In the context of this project, an automated pipeline employing libraries like AutoClean \cite{AUTOCLEAN} was developed to perform these tasks systematically, ensuring that the datasets used by the recommender agents are clean, consistent, and reliable.