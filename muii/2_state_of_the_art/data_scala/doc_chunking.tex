One of the primary limitations of \acp{llm} is their finite context window, which restricts the amount of information that can be processed in a single query. To apply these models to large documents or extensive knowledge bases, a technique known as chunking is employed, particularly within \ac{rag} frameworks \cite{SOTA-RAG-SURVEY}.

Chunking is the process of breaking down large texts into smaller, semantically meaningful segments. The goal is to create chunks that are small enough to fit within the model's context window, yet large enough to retain their original meaning and relevance. The strategy used for chunking---whether it involves fixed-size splits, sentence-based division, or more advanced content-aware methods---has a direct and significant impact on the quality of the retrieval process. Effective chunking ensures that the information retrieved and presented to the \ac{llm} is coherent and relevant, which is fundamental to generating accurate and contextually-aware responses \cite{SOTA-ADVANCED-RAG}. Although this is a primary technique for \acs{llm} contextualization, it was not employed in this project due to the tabular nature of the datasets.